import requests
import csv
import re

# Set your GitLab project ID and access token
PROJECT_ID = 41299063
ACCESS_TOKEN = "glpat-XsRzi9SD8yHJyMavhhh3"

# Define the API endpoint URL
BASE_URL = f"https://gitlab.com/api/v4/projects/{PROJECT_ID}"

# Function to retrieve notes for a specific issue
def get_notes_for_issue(issue_id):
    notes_url = f"{BASE_URL}/issues/{issue_id}/notes"
    response = requests.get(notes_url, headers={"PRIVATE-TOKEN": ACCESS_TOKEN})
    if response.status_code == 200:
        return response.json()
    return []

# Create a CSV file and write the data
with open("all_issues_comments_grouped.csv", mode="w", newline="", encoding="utf-8") as csv_file:
    writer = csv.writer(csv_file)

    # Get a list of all issues
    issues_url = f"{BASE_URL}/issues?per_page=100"  # Adjust per_page as needed
    page = 1
    while True:
        response = requests.get(issues_url + f"&page={page}", headers={"PRIVATE-TOKEN": ACCESS_TOKEN})
        if response.status_code != 200:
            print(f"Failed to retrieve issues. Status code: {response.status_code}")
            break
        issues_data = response.json()
        if not issues_data:
            break  # No more issues to retrieve

        # Create a dictionary to store upload paths for each issue
        upload_paths_dict = {}
        
        for issue in issues_data:
            issue_id = issue["iid"]
            notes = get_notes_for_issue(issue_id)
            upload_paths = []

            for note in notes:
                body = note["body"]
                uploads = re.findall(r'/uploads/[^)]*', body)
                upload_paths.extend(uploads)

            # Add the prefix to each upload path
            prefix = 'file://csvimport/'
            upload_paths = [prefix + path for path in upload_paths]

            upload_paths_dict[issue_id] = upload_paths

        # Determine the maximum number of upload paths across all issues
        max_upload_paths = max(len(paths) for paths in upload_paths_dict.values())

        # Create headers for Upload Paths
        headers = ["Issue ID"] + [f"Upload Paths {i+1}" for i in range(max_upload_paths)]
        writer.writerow(headers)

        # Write data to CSV
        for issue_id, paths in upload_paths_dict.items():
            paths += [''] * (max_upload_paths - len(paths))  # Pad with empty strings
            row_data = [issue_id] + paths
            writer.writerow(row_data)

        page += 1

print("Data has been saved to all_issues_comments_grouped.csv")
